{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "#pyspark的入口对象 SparkContext\n",
    "#导包\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#创建sparkconf对象(配置对象) 运行模式 和 name 下面这个反斜杠是换行符的意思 这是一个链式调用 . . 多个点多次调用\n",
    "conf = SparkConf().setMaster(\"local[*]\").\\\n",
    "    setAppName(\"test_spark_app\")\n",
    "\n",
    "#基于sparkconf类对象创建sparkcontext对象\n",
    "sc = SparkContext(conf=conf) #是把上面的conf传入\n",
    "\n",
    "#打印pyspark的运行版本\n",
    "print(sc.version)\n",
    "\n",
    "#停止sparkcontext对象的运行（停止spark程序）\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m\n\u001b[0;32m      6\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext(conf\u001b[39m=\u001b[39mconf)\n\u001b[0;32m      8\u001b[0m \u001b[39m#用parallelize来将Py对象转换到RDD对象\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# rdd1 = sc.parallelize([1,2,3,4,5])\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# rdd2 = sc.parallelize((1,2,3,4,5))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[39m#读取文件\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m rdd_text \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mtextFile(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m桌面\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mvs_all\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mPcode\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mtest.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(rdd_text\u001b[39m.\u001b[39mcollect())\n\u001b[0;32m     26\u001b[0m sc\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Program Files\\python\\lib\\site-packages\\pyspark\\context.py:986\u001b[0m, in \u001b[0;36mSparkContext.textFile\u001b[1;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[0;32m    925\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtextFile\u001b[39m(\n\u001b[0;32m    926\u001b[0m     \u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, minPartitions: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, use_unicode: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    927\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RDD[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m    928\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[39m    Read a text file from HDFS, a local file system (available on all\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[39m    nodes), or any Hadoop-supported file system URI, and return it as an\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[39m    ['aa', 'bb', 'cc', 'x', 'y', 'z']\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     minPartitions \u001b[39m=\u001b[39m minPartitions \u001b[39mor\u001b[39;00m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefaultParallelism, \u001b[39m2\u001b[39m)\n\u001b[0;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m RDD(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39mtextFile(name, minPartitions), \u001b[39mself\u001b[39m, UTF8Deserializer(use_unicode))\n",
      "File \u001b[1;32mc:\\Program Files\\python\\lib\\site-packages\\pyspark\\context.py:627\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    616\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefaultParallelism\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    617\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39m    True\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc()\u001b[39m.\u001b[39mdefaultParallelism()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "#数据容器转RDD对象\n",
    "from pyspark import SparkConf,SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"test_spark\")\n",
    "if 'sc' not in globals(): #只允许有一条sparkcontext实例\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "#用parallelize 平行 来将Py对象转换到RDD对象\n",
    "# rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "# rdd2 = sc.parallelize((1,2,3,4,5))\n",
    "# rdd3 = sc.parallelize(\"123asd\")\n",
    "# rdd4 = sc.parallelize({1,2,3,4,5})\n",
    "# rdd5 = sc.parallelize({\"key1\":1,\"key2\":2})\n",
    "\n",
    "#若查看RDD有什么内容，则用collect方法\n",
    "# print(rdd1.collect())\n",
    "# print(rdd2.collect())\n",
    "# print(rdd3.collect())\n",
    "# print(rdd4.collect())\n",
    "# #字典只把key带进来了\n",
    "# print(rdd5.collect())\n",
    "\n",
    "#读取文件\n",
    "rdd_text = sc.textFile(r\"D:\\桌面\\vs_all\\Pcode\\test.txt\")\n",
    "print(rdd_text.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=test_spark, master=local[*]) created by __init__ at C:\\Users\\MasterSun\\AppData\\Local\\Temp\\ipykernel_18132\\3628341755.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf,SparkContext\n\u001b[0;32m      4\u001b[0m conf \u001b[39m=\u001b[39m SparkConf()\u001b[39m.\u001b[39msetMaster(\u001b[39m\"\u001b[39m\u001b[39mlocal[*]\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msetAppName(\u001b[39m\"\u001b[39m\u001b[39mtest_spark\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m sc \u001b[39m=\u001b[39m SparkContext(conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m      6\u001b[0m \u001b[39m#用parallelize来将Py对象转换到RDD对象\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# rdd1 = sc.parallelize([1,2,3,4,5])\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# rdd2 = sc.parallelize((1,2,3,4,5))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[39m#读取文件\u001b[39;00m\n\u001b[0;32m     22\u001b[0m rdd_text \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mtextFile(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m桌面\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mvs_all\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mPcode\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtest.txt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\python\\lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\python\\lib\\site-packages\\pyspark\\context.py:445\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m     callsite \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_callsite\n\u001b[0;32m    444\u001b[0m     \u001b[39m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot run multiple SparkContexts at once; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexisting SparkContext(app=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, master=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m created by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[0;32m    450\u001b[0m             currentAppName,\n\u001b[0;32m    451\u001b[0m             currentMaster,\n\u001b[0;32m    452\u001b[0m             callsite\u001b[39m.\u001b[39mfunction,\n\u001b[0;32m    453\u001b[0m             callsite\u001b[39m.\u001b[39mfile,\n\u001b[0;32m    454\u001b[0m             callsite\u001b[39m.\u001b[39mlinenum,\n\u001b[0;32m    455\u001b[0m         )\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=test_spark, master=local[*]) created by __init__ at C:\\Users\\MasterSun\\AppData\\Local\\Temp\\ipykernel_18132\\3628341755.py:5 "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mPYSPARK_PYTHON\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProgram Files\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpython.exe\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39m#准备一个RDD\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m rdd \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mparallelize([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m4\u001b[39;49m,\u001b[39m5\u001b[39;49m])\n\u001b[0;32m      8\u001b[0m \u001b[39m#通过map方法把每个数字都乘10\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(data):\n",
      "File \u001b[1;32mc:\\Program Files\\python\\lib\\site-packages\\pyspark\\context.py:780\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallelize\u001b[39m(\u001b[39mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RDD[T]:\n\u001b[0;32m    749\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[39m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[39m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[39m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 780\u001b[0m     numSlices \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(numSlices) \u001b[39mif\u001b[39;00m numSlices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefaultParallelism\n\u001b[0;32m    781\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(c, \u001b[39mrange\u001b[39m):\n\u001b[0;32m    782\u001b[0m         size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(c)\n",
      "File \u001b[1;32mc:\\Program Files\\python\\lib\\site-packages\\pyspark\\context.py:627\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    616\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefaultParallelism\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    617\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39m    True\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc()\u001b[39m.\u001b[39mdefaultParallelism()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = \"C:\\Program Files\\python\\python.exe\"\n",
    "\n",
    "#准备一个RDD\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "\n",
    "\n",
    "#通过map方法把每个数字都乘10\n",
    "def func(data):\n",
    "    return 10*data\n",
    "\n",
    "#每个数字都传入func\n",
    "rdd_ans = rdd.map(func)\n",
    "\n",
    "print(rdd_ans.collect)\n",
    "#(T) -> U 传入一个T 返回一个U\n",
    "#(T) -> T 传入和传出一个类型\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7d32af1e689683b59f84af5e00ff8511e054604b74fc1830ce1b819250cf76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
